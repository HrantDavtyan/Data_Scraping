{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nThis website provides materials covered during the Data Scraping course, Summer 2017.\nThe course is taught by Hrant Davtyan as an elective course available to 1st year MS Economics students at the American University of Armenia (AUA). \n\n\nToolbox\n\n\nDuring the course students will learn and use several tools necessary for completing a Data Scraping project as listed below:\n\n\n\n\nSublime Text Editor\n - A simple yet powerful (user friendly interface + amazing performance) text editor. During the course Sublime will be used for creating and editing HTML, CSS, XML and JSON documents.\n\n\nSelector Gadget\n - A google chrome extension that helps to easily discover CSS selectors for the elements on the webpage.\n\n\nJSON formatter\n - A google chrome extension which makes the JSON representation indented and highlighted (when viewed directly inside the browser).\n\n\nRegex search\n - Another chrome extension which provides the opportunity of running a search on the webpage using regular extensions directly inside the borwser.\n\n\nOnline regex tester\n - an online tool for testing a regular expression on a sample text typed by the user. Also provides quick reference sheet and interactive explanation of the expression being tested.\n\n\nAnaconda Python 2.7\n - Python powered open data science platform, which comes toegther with Jupyter notebooks, Spider IDE and some of the most popular Python libraries preinstalled. During the course several python packages will be used. The list of packages (including thsoe preinstalled by Anaconda) is available below.\n\n\n\n\nRequired packages\n\n\n\n\nrequests\n\n\nre\n\n\nlxml\n\n\nhtml5lib\n\n\nbeautifulsoup\n\n\nscrapy\n\n\nselenium\n\n\nnumpy\n\n\npandas\n\n\nsklearn\n\n\nstatsmodels\n\n\npandas-datareader\n\n\npython-linkedin\n\n\nmarkovbot\n\n\ngooglemaps\n\n\npafy\n\n\nQuandl\n\n\n\n\nThe packages \njson\n, \ncsv\n, \ntime\n and \nurllib2\n are also required, yet they come preinstalled with Python 2.7.\n\n\nInstallation\n\n\nTo install the above provided packages please download requirements.txt to your local directory and run the following command in the command prompt:\n\n\npip install -r requirements.txt", 
            "title": "1. Administration"
        }, 
        {
            "location": "/#overview", 
            "text": "This website provides materials covered during the Data Scraping course, Summer 2017.\nThe course is taught by Hrant Davtyan as an elective course available to 1st year MS Economics students at the American University of Armenia (AUA).", 
            "title": "Overview"
        }, 
        {
            "location": "/#toolbox", 
            "text": "During the course students will learn and use several tools necessary for completing a Data Scraping project as listed below:   Sublime Text Editor  - A simple yet powerful (user friendly interface + amazing performance) text editor. During the course Sublime will be used for creating and editing HTML, CSS, XML and JSON documents.  Selector Gadget  - A google chrome extension that helps to easily discover CSS selectors for the elements on the webpage.  JSON formatter  - A google chrome extension which makes the JSON representation indented and highlighted (when viewed directly inside the browser).  Regex search  - Another chrome extension which provides the opportunity of running a search on the webpage using regular extensions directly inside the borwser.  Online regex tester  - an online tool for testing a regular expression on a sample text typed by the user. Also provides quick reference sheet and interactive explanation of the expression being tested.  Anaconda Python 2.7  - Python powered open data science platform, which comes toegther with Jupyter notebooks, Spider IDE and some of the most popular Python libraries preinstalled. During the course several python packages will be used. The list of packages (including thsoe preinstalled by Anaconda) is available below.", 
            "title": "Toolbox"
        }, 
        {
            "location": "/#required-packages", 
            "text": "requests  re  lxml  html5lib  beautifulsoup  scrapy  selenium  numpy  pandas  sklearn  statsmodels  pandas-datareader  python-linkedin  markovbot  googlemaps  pafy  Quandl   The packages  json ,  csv ,  time  and  urllib2  are also required, yet they come preinstalled with Python 2.7.", 
            "title": "Required packages"
        }, 
        {
            "location": "/#installation", 
            "text": "To install the above provided packages please download requirements.txt to your local directory and run the following command in the command prompt:  pip install -r requirements.txt", 
            "title": "Installation"
        }, 
        {
            "location": "/materials/", 
            "text": "Week 1 - Intro to HTML/CSS\n\n\nThis component includes materials covered during the 1st week of the course (2 lectures of 2.5 hour duration).\nDuring the first lecture the course objective, structure, content, tools and frame were covered.\nDuring the 2nd lecture HTML/CSS were introduced and how to host them on GitHub as a webpage.\n\n\n\n\nAll materials (slides + HTML/CSS files)\n\n\n\n\nWeek 2 - Intro to Python\n\n\nThis component includes materials covered during the 2nd week of the course (2 lectures of 2.5 hour duration). Python 2.7. programming language was introduced and Jupyter notebooks were used for executing the code. Topics covered include data types, control flow, functions, I/O. Packages covered include NumPy, Pandas, Matplotlib, Pandas DataReader.\n\n\n\n\nIntro to Python 1: the basics\n\n\nIntro to Python 2: functions\n\n\nIntro to Python 3: datareader\n\n\n\n\nWeek 3 - Regular Dealing with\n\n\nThis component http://nbviewer.jupyter.org/github/HrantDavtyan/Data_Scraping/blob/master/Week%203/JSON_part_1.ipynbincludes materials covered during the 3rd week of the course (2 lectures of 2.5 hour duration).Dealing  Regular expressions, native Python, as well as pandas imported read and write functions were introduced. Sublime text editor and Jupyter notebooks were used for executing the code. Topics covered include reading documents (csv, json, html, txt), writing files (csv, json), Regular expressions, parsing json as well as creating simple bot operating on Markvo chains. Packages covered include Pandas, JSON, RegEx, MarkovBot.\n\n\n\n\nReading and writing files (pure)\n\n\nReading and Writing files (Pandas)\n\n\nRegEx 1: financier, e-mail\n\n\nRegEx 2: requests, HTML\n\n\nDealing with JSON files 1\n\n\nDealing with JSON files 2\n\n\n\n\nWeek 4 - BeautifulSoup\n\n\nThis component includes materials covered during the 4th week of the course (2 lectures of 2.5 hour duration). Scraping HTML webpages with BeautifulSoup was introduced. Sublime text editor and Jupyter notebooks were used for executing the code. Topics covered include reading and writing CSV files using the CSV module, reading and writing JSON files using the JSON modeule, web scraping with BeautifulSoup using the HTML tags/CSS selectors. Packages covered include JSON, CSV, urllib, BeautifulSoup.\n\n\n\n\nBeautifulSoup 1: My webpage\n\n\nBeautifulSoup 2: Bloomberg\n\n\nBeautifulSoup 3: www.careercenter.am\n\n\nBeautifulSoup 4: Navigation\n\n\nCraftcans.com - Pandas\n\n\nCraftcans.com - BeautifulSoup\n\n\nCraftcans.com - cleaning\n\n\n\n\nWeek 5 - XML files and LXML\n\n\nThis component includes materials covered during the 5th week of the course (2 lectures of 2.5 hour duration). Overview of the first 4 week material was provided followed by a midterm exam (exam paper available in this folder). During the 2nd lecture alternative libraries for scraping were covered (urllib2 as an alternative to requests, and lxml as an alternative to BeautifulSoup). Sublime text editor and Jupyter notebooks were used for executing the code. Topics covered include working with XML documents, scraping the web using XPath, differences between lxml and BeautifulSoup. Packages covered include LXML and urllib2.\n\n\n\n\nAlternative libraries for scraping\n\n\nWorking with XML documents\n\n\n\n\nWeek 6 - Scrapy\n\n\nThis component includes materials covered during the 6th week of the course (2 lectures of 2.5 hour duration). Scrapy scraping and crawling framework was introduced following the official documentation. Sublime text editor and command prompt (directly) were used for executing the code (also Jupyter notebooks were used for sharing the code). Topics covered include scraping quotes, putting timer, naming the agent, crawling over several pages. Packages covered include Scrapy, pacf.\n\n\n\n\nScrapy 1: using inside shell\n\n\nScrapy 2: developing a spider\n\n\nScrapy 3: developing a crawler\n\n\nYouTube API: unauthorized access\n\n\n\n\nWeek 7 - APIs\n\n\nWeek 8 - Beyond scraping", 
            "title": "2. Materials"
        }, 
        {
            "location": "/materials/#week-1-intro-to-htmlcss", 
            "text": "This component includes materials covered during the 1st week of the course (2 lectures of 2.5 hour duration).\nDuring the first lecture the course objective, structure, content, tools and frame were covered.\nDuring the 2nd lecture HTML/CSS were introduced and how to host them on GitHub as a webpage.   All materials (slides + HTML/CSS files)", 
            "title": "Week 1 - Intro to HTML/CSS"
        }, 
        {
            "location": "/materials/#week-2-intro-to-python", 
            "text": "This component includes materials covered during the 2nd week of the course (2 lectures of 2.5 hour duration). Python 2.7. programming language was introduced and Jupyter notebooks were used for executing the code. Topics covered include data types, control flow, functions, I/O. Packages covered include NumPy, Pandas, Matplotlib, Pandas DataReader.   Intro to Python 1: the basics  Intro to Python 2: functions  Intro to Python 3: datareader", 
            "title": "Week 2 - Intro to Python"
        }, 
        {
            "location": "/materials/#week-3-regular-dealing-with", 
            "text": "This component http://nbviewer.jupyter.org/github/HrantDavtyan/Data_Scraping/blob/master/Week%203/JSON_part_1.ipynbincludes materials covered during the 3rd week of the course (2 lectures of 2.5 hour duration).Dealing  Regular expressions, native Python, as well as pandas imported read and write functions were introduced. Sublime text editor and Jupyter notebooks were used for executing the code. Topics covered include reading documents (csv, json, html, txt), writing files (csv, json), Regular expressions, parsing json as well as creating simple bot operating on Markvo chains. Packages covered include Pandas, JSON, RegEx, MarkovBot.   Reading and writing files (pure)  Reading and Writing files (Pandas)  RegEx 1: financier, e-mail  RegEx 2: requests, HTML  Dealing with JSON files 1  Dealing with JSON files 2", 
            "title": "Week 3 - Regular Dealing with"
        }, 
        {
            "location": "/materials/#week-4-beautifulsoup", 
            "text": "This component includes materials covered during the 4th week of the course (2 lectures of 2.5 hour duration). Scraping HTML webpages with BeautifulSoup was introduced. Sublime text editor and Jupyter notebooks were used for executing the code. Topics covered include reading and writing CSV files using the CSV module, reading and writing JSON files using the JSON modeule, web scraping with BeautifulSoup using the HTML tags/CSS selectors. Packages covered include JSON, CSV, urllib, BeautifulSoup.   BeautifulSoup 1: My webpage  BeautifulSoup 2: Bloomberg  BeautifulSoup 3: www.careercenter.am  BeautifulSoup 4: Navigation  Craftcans.com - Pandas  Craftcans.com - BeautifulSoup  Craftcans.com - cleaning", 
            "title": "Week 4 - BeautifulSoup"
        }, 
        {
            "location": "/materials/#week-5-xml-files-and-lxml", 
            "text": "This component includes materials covered during the 5th week of the course (2 lectures of 2.5 hour duration). Overview of the first 4 week material was provided followed by a midterm exam (exam paper available in this folder). During the 2nd lecture alternative libraries for scraping were covered (urllib2 as an alternative to requests, and lxml as an alternative to BeautifulSoup). Sublime text editor and Jupyter notebooks were used for executing the code. Topics covered include working with XML documents, scraping the web using XPath, differences between lxml and BeautifulSoup. Packages covered include LXML and urllib2.   Alternative libraries for scraping  Working with XML documents", 
            "title": "Week 5 - XML files and LXML"
        }, 
        {
            "location": "/materials/#week-6-scrapy", 
            "text": "This component includes materials covered during the 6th week of the course (2 lectures of 2.5 hour duration). Scrapy scraping and crawling framework was introduced following the official documentation. Sublime text editor and command prompt (directly) were used for executing the code (also Jupyter notebooks were used for sharing the code). Topics covered include scraping quotes, putting timer, naming the agent, crawling over several pages. Packages covered include Scrapy, pacf.   Scrapy 1: using inside shell  Scrapy 2: developing a spider  Scrapy 3: developing a crawler  YouTube API: unauthorized access", 
            "title": "Week 6 - Scrapy"
        }, 
        {
            "location": "/materials/#week-7-apis", 
            "text": "", 
            "title": "Week 7 - APIs"
        }, 
        {
            "location": "/materials/#week-8-beyond-scraping", 
            "text": "", 
            "title": "Week 8 - Beyond scraping"
        }, 
        {
            "location": "/homework/", 
            "text": "Homework 1\n\n\nDevelop and style your own website using HTML and CSS, which should at least include the following components:\n\n\n\n\nA heading (which should change color and font once hovered) and at least 3 paragraphs (with at least one \n  hyperlink inside the text, similar to \"AUA\" from the lecture files),\n\n\nAn image (preferably your photo, but other related picture/image is also accepted) with  an alternative\n  text and sized using height and width,\n\n\nAn ordered or unordered list (of hobbies, skills, extra-curriculum activities or anything else related),\n  which should be inside a \n tag with class=\"my_list\" argument (or similar),\n\n\nA table (not necessarily with visible borders, can be used to structure the overall content of the website),\n\n\nA button (next/submit or else related), which will navigate to another page created by you once clicked.\n\n\n\n\nEach point above is worth one mark. The overall homework is worth 5 marks (and 5% of final grade). It should\nsolely be your own work typed from scratch. Although you are encouraged to look through the files created in \nclass, copying a code from that files is strictly prohibited and will result in decrease of grade.\n\n\nAnything additional (borders, color, input forms or else) is highly welcomed.\n\n\nHomework 2\n\n\n1) Construct a function, which will ask the user to input several numbers separated by commas and will calculate\n   their average. (e.g. if the user inputs 3,5,7 the function must result in 5)\n\n\n2) Upgrade the function above to also ask the user the number of occurrences to calculate the average for.\n   (e.g. if the user inputs 1,3,5,7,9 and as the second argument to function inputs 2, the function must result\n   in (7+9)/2 = 8. If the user inputs the same numbers as a first argument, but inputs 3 as the 2nd, then\n   the function must result in (5+7+9)/3=7)\n\n\n3) Construct a function, which will generate a random number between 1 and 100 (both included). If this number is\n   between 50 and 100 (not included) the function returns \"Win\", if it is between 1 and 50 (both included)\n   the function returns \"Loss\" and if it is exactly 100 then the function returns \"Draw\". (Hint: to generate a\n   random number, one should first import a package called random (i.e. import random) and then use the function\n   randint from there (e.g. random.randint(x,y), where x=1 and y=100 in our case)\n\n\n4) Create a list of 3 stocks (choose whatever stocks you want, e.g. [\"IBM\",\"AAPL\",\"MSFT\"]). Create a for loop\n   that will iterate over the list, get the data of relevant stock and print the first 7 rows of the data.\n\n\n5) Upgrade the for loop above.  Now, instead of printing the data, the for loop should iterate over the list,\n   get the data and plot it.\n\n\nEach point above is worth one mark. The overall homework is worth 5 marks (and 5% of final grade).\nYou are encouraged to look through classroom materials, discuss with other students, yet your submission should\nsolely be your own work.\n\n\nAnything additional (plot coloring, customization etc.) is highly welcomed.\n\n\nHomework 3\n\n\n1) Use read_html() function from the pandas library inside a for loop to get and print exchange rate data from\n   rate.am for a week starting on June 1st and ending on 7th of June, 2017 included. The necessary steps to take\n   are: 1) create the list of URLs to scrape, 2) create a for loop to iterate over the elements of that list and\n   3) receive the data from each of them and 4) print it. (Hint: you may try for 2 URLs first, and then go for 7).\n\n\n2) Use regular expressions to match (find) and print the value of S\nP500 index from the Bloomberg website\n   (link: https://www.bloomberg.com/quote/SPX:IND, at the time of writing this the value of the index is\n   2,434.51). Your regular expression must match any value of S\nP500 even if it changes (e.g. whether it becomes\n   1 or even 1,500,000.005, for example).\n\n\n3) Create a for loop that will iterate over a given JSON file and print the keys followed by their values.\n   The file (input string, which yet needs to be converted to JSON, as done in the classroom) and the expected\n   output can be found in this Jupyter notebook: https://goo.gl/4IT7xG.\n   NOTE: not more than one \"if\" and one \"else\" statements are allowed to use inside the for loop.\n\n\n4) Download the AirPassengers.csv file from the Datasets folder in the Moodle, read that file to python and\n   plot the \"Passengers\" column.\n\n\n5) Use regular expressions to find the URL (the hyperlink) under the Next button on this webpage:\n   http://quotes.toscrape.com/. \n\n\nEach point above is worth one mark. The overall homework is worth 5 marks (and 5% of the final grade).\nYou are encouraged to look trough the classroom materials, discuss with other students, yet your submission\nshould solely be your own work.\n\n\nAnything additional is highly welcomed (yet being highly welcomed doesn't generate additional points on homework).\n\n\nHomework 4\n\n\nThe most trending YouTube videos (https://www.youtube.com/feed/trending) must be scraped (it is allowed,\naccording to robots.txt). You must get the following data on each movie:\n\n\n\n\nURL,\n\n\ntitle,\n\n\nduration,\n\n\nusername,\n\n\nviews.\n\n\n\n\n1) Get the data using requests and BeautifulSoup (2 points),\n\n\n2) Get the data using urllib2 and lxml (2 points),\n\n\n3) Save the data into a CSV and a JSON file (1 point).\n\n\nPlease, provide Jupyter notebooks (or .py files) separately for point 1 (BeautifulSoup) and 2 (lxml).\nMake sure to receive data on all trending movies. Anything additional (e.g. saving data also as a XML file\nor calculating the average number of views) is highly welcomed. \n\n\nHomework 5\n\n\nHomework 6", 
            "title": "3. Homework"
        }, 
        {
            "location": "/homework/#homework-1", 
            "text": "Develop and style your own website using HTML and CSS, which should at least include the following components:   A heading (which should change color and font once hovered) and at least 3 paragraphs (with at least one \n  hyperlink inside the text, similar to \"AUA\" from the lecture files),  An image (preferably your photo, but other related picture/image is also accepted) with  an alternative\n  text and sized using height and width,  An ordered or unordered list (of hobbies, skills, extra-curriculum activities or anything else related),\n  which should be inside a   tag with class=\"my_list\" argument (or similar),  A table (not necessarily with visible borders, can be used to structure the overall content of the website),  A button (next/submit or else related), which will navigate to another page created by you once clicked.   Each point above is worth one mark. The overall homework is worth 5 marks (and 5% of final grade). It should\nsolely be your own work typed from scratch. Although you are encouraged to look through the files created in \nclass, copying a code from that files is strictly prohibited and will result in decrease of grade.  Anything additional (borders, color, input forms or else) is highly welcomed.", 
            "title": "Homework 1"
        }, 
        {
            "location": "/homework/#homework-2", 
            "text": "1) Construct a function, which will ask the user to input several numbers separated by commas and will calculate\n   their average. (e.g. if the user inputs 3,5,7 the function must result in 5)  2) Upgrade the function above to also ask the user the number of occurrences to calculate the average for.\n   (e.g. if the user inputs 1,3,5,7,9 and as the second argument to function inputs 2, the function must result\n   in (7+9)/2 = 8. If the user inputs the same numbers as a first argument, but inputs 3 as the 2nd, then\n   the function must result in (5+7+9)/3=7)  3) Construct a function, which will generate a random number between 1 and 100 (both included). If this number is\n   between 50 and 100 (not included) the function returns \"Win\", if it is between 1 and 50 (both included)\n   the function returns \"Loss\" and if it is exactly 100 then the function returns \"Draw\". (Hint: to generate a\n   random number, one should first import a package called random (i.e. import random) and then use the function\n   randint from there (e.g. random.randint(x,y), where x=1 and y=100 in our case)  4) Create a list of 3 stocks (choose whatever stocks you want, e.g. [\"IBM\",\"AAPL\",\"MSFT\"]). Create a for loop\n   that will iterate over the list, get the data of relevant stock and print the first 7 rows of the data.  5) Upgrade the for loop above.  Now, instead of printing the data, the for loop should iterate over the list,\n   get the data and plot it.  Each point above is worth one mark. The overall homework is worth 5 marks (and 5% of final grade).\nYou are encouraged to look through classroom materials, discuss with other students, yet your submission should\nsolely be your own work.  Anything additional (plot coloring, customization etc.) is highly welcomed.", 
            "title": "Homework 2"
        }, 
        {
            "location": "/homework/#homework-3", 
            "text": "1) Use read_html() function from the pandas library inside a for loop to get and print exchange rate data from\n   rate.am for a week starting on June 1st and ending on 7th of June, 2017 included. The necessary steps to take\n   are: 1) create the list of URLs to scrape, 2) create a for loop to iterate over the elements of that list and\n   3) receive the data from each of them and 4) print it. (Hint: you may try for 2 URLs first, and then go for 7).  2) Use regular expressions to match (find) and print the value of S P500 index from the Bloomberg website\n   (link: https://www.bloomberg.com/quote/SPX:IND, at the time of writing this the value of the index is\n   2,434.51). Your regular expression must match any value of S P500 even if it changes (e.g. whether it becomes\n   1 or even 1,500,000.005, for example).  3) Create a for loop that will iterate over a given JSON file and print the keys followed by their values.\n   The file (input string, which yet needs to be converted to JSON, as done in the classroom) and the expected\n   output can be found in this Jupyter notebook: https://goo.gl/4IT7xG.\n   NOTE: not more than one \"if\" and one \"else\" statements are allowed to use inside the for loop.  4) Download the AirPassengers.csv file from the Datasets folder in the Moodle, read that file to python and\n   plot the \"Passengers\" column.  5) Use regular expressions to find the URL (the hyperlink) under the Next button on this webpage:\n   http://quotes.toscrape.com/.   Each point above is worth one mark. The overall homework is worth 5 marks (and 5% of the final grade).\nYou are encouraged to look trough the classroom materials, discuss with other students, yet your submission\nshould solely be your own work.  Anything additional is highly welcomed (yet being highly welcomed doesn't generate additional points on homework).", 
            "title": "Homework 3"
        }, 
        {
            "location": "/homework/#homework-4", 
            "text": "The most trending YouTube videos (https://www.youtube.com/feed/trending) must be scraped (it is allowed,\naccording to robots.txt). You must get the following data on each movie:   URL,  title,  duration,  username,  views.   1) Get the data using requests and BeautifulSoup (2 points),  2) Get the data using urllib2 and lxml (2 points),  3) Save the data into a CSV and a JSON file (1 point).  Please, provide Jupyter notebooks (or .py files) separately for point 1 (BeautifulSoup) and 2 (lxml).\nMake sure to receive data on all trending movies. Anything additional (e.g. saving data also as a XML file\nor calculating the average number of views) is highly welcomed.", 
            "title": "Homework 4"
        }, 
        {
            "location": "/homework/#homework-5", 
            "text": "", 
            "title": "Homework 5"
        }, 
        {
            "location": "/homework/#homework-6", 
            "text": "", 
            "title": "Homework 6"
        }, 
        {
            "location": "/exams/", 
            "text": "Midterm\n\n\nMidterm Exam\n\n\n19 June 2017\n\n\n\nExam Duration: 1 hour 15 minutes\n\n\n\nPart 1 \u2013 Multiple-choice questions (10 points)\n\n\n\nQuestion 1\n\n\nThe following code is used for reading a local file from the current directory. Find the mistake.\n\n\nimport json\n\nwith open(\nsome_file.json\n,\nr\n) as f:\n    my_file = json.dumps(f)\n\n\n\n\n\n\nsome_file.json was opened but not closed, which is compulsory thing to do,\n\n\nthe dumps()is used for writing files and cannot be applied to read-only files,\n\n\nf is a file name, which means it should be provided inside quotes as \"f\",\n\n\nsingle equal sign is used for assign and not for comparison purposes, which means the last line should be changed to my_file == json.dumps(f).\n\n\n\n\nQuestion 2\n\n\nWhich of the following statements is wrong?\n\n\n\n\nread_html() function from the Pandas library can be used for reading tables in HTML.\n\n\nfindall() function from the re (regular expressions) library returns a list of matching strings.\n\n\nIf an expression to be matched includes at least one digit, then both \"\\d*\" and \"\\d+\" regular expressions will match the expression.\n\n\nread_csv() function from the Pandas library can be used exclusively for reading CSV files.\n\n\n\n\nQuestion 3\n\n\nWhat will the following code print once executed?\n\n\nmy_list = []\n\nfor i in range(4):\n    my_list.append(str(i) + str( len(my_list) ) )\n\nprint(my_list[3])\n\n\n\n\n\n\nNone\n\n\n2\n\n\n6\n\n\n33\n\n\n\n\nQuestion 4\n\n\nWhich of the following statements is not true about scraping?\n\n\n\n\nScrapers broke over time, as websites change their structure.\n\n\nToo aggressive (e.g. frequent) scraping may cause troubles for the website.\n\n\nScraped data may not be perfectly clean and need some additional processing.\n\n\nAny data that can be publicly accessed can also be freely scraped.\n\n\n\n\nQuestion 5\n\n\nWhich of the following regular expressions will correctly match the bold component in the following text:\n\n\n\"Dear students, please be informed that our midterm exam will be held on Monday, \nJune 19th, 15:30-16:45\n.\"\n\n\n\n\n\\s([A-Z][a-z]+\\s[0-9]{1,2}[a-z]{2}.+).\n\n\n\\s[A-Z][a-z]+\\s[0-9]{1,2}[a-z]{2}.+.\n\n\n[A-Z][a-z]+\\s\\S+.+\n\n\n,\\s\\S+\\s\\S+\\s\\S+\n\n\n\n\nPart 2 \u2013 Short-answer questions (10 points)\n\n\n\nMETRIC is a Yerevan-based think-tank specialized in data-driven research and consulting. It has a website that can be reached at \nhttp://metric.am/\n. You are asked to get data from this website using scraping techniques.\n\n\n\n\n\n\nPlease provide the link that you would check first before scraping METRIC's website.\n\n\n\n\n\n\nAssuming the link you provided above shows the following information:\n\n\n\n\n\n\nUser-agent: sogou web spider\nDisallow: /\n\n\n\n\nWhat conclusions can you make regarding scraping after reading the information above?\n\n\n\n\nThe website continuously posts paragraphs from Wikipedia. The following is a copy from the page's HTML source of the part where a paragraph from Wikipedia was copied.\n\n\n\n\np class = \nwikipedia\n id = \nranking_armenia\nThe economy of Armenia is ranked 132nd in the world,\nwith a nominal gross domestic product (GDP) of $10.561 billion per annum. It is also the 129th largest\nin the world by purchasing power parity (PPP), at $25.329 billion per annum.\n/p\n\n\n\n\n\nProvide a Python code (using requests and BeautifulSoup), which will find the paragraph above and return its text to some variable.\n\n\n\n\n\n\nProvide a regular expression that will find and return the 2nd ranking (129th) from the same text above. The expression must work even if the ranking changes significantly.\n\n\n\n\n\n\nThe paragraph is followed by a \"Read more\" text, which redirects to the original Wikipedia article page once clicked (the article page is: \nhttps://en.wikipedia.org/wiki/Economy_of_Armenia\n). Provide the HTML source for the \"Read more\" text.\n\n\n\n\n\n\nPart 3 \u2013 Open-ended problem (10 points)\n\n\n\nwww.tert.am\n is an Armenian general-purpose online newspaper. The very first page has a table-like component in the bottom, which presents the news feed: hyperlinked titles of all published news in a day (see screenshot below).\n\n\n\n\nThe inspected HTML source for this component of the page looks like this:\n\n\n\n\nNote: the first \ndiv\n is responsible for the whole table-like component, then each \np\n and \ndiv\n inside provide the content in that component.\n\n\nPlease, write down a Python code, which will go to \nhttp://www.tert.am/am/\n, find this table-like component and get from there the date (not time), the title and the link of each separate observation (i.e. published news), save this information in a JSON format and save the file into a JSON file. Please, provide careful explanation of all the steps you would take before, during and after writing this scraper code. Please, provide reasoning/explanation behind all components of your code (e.g. what each component is doing). You may provide it as Python comments or as a separate paragraph.\n\n\nFinal", 
            "title": "4. Exams"
        }, 
        {
            "location": "/exams/#midterm", 
            "text": "", 
            "title": "Midterm"
        }, 
        {
            "location": "/exams/#final", 
            "text": "", 
            "title": "Final"
        }, 
        {
            "location": "/resources/", 
            "text": "Chrome extensions\n\n\n\n\nSelector Gadget\n - A google chrome extension that helps to easily discover CSS selectors for the elements on the webpage.\n\n\nJSON formatter\n - A google chrome extension which makes the JSON representation indented and highlighted (when viewed directly inside the browser).\n\n\nRegex search\n - Another chrome extension which provides the opportunity of running a search on the webpage using regular extensions directly inside the borwser.\n\n\n\n\nFirefox extensions\n\n\n\n\nJSON view\n - A Firefox extension which makes the JSON representation indented and highlighted (when viewed directly inside the browser).\n\n\n\n\nWebsites\n\n\n\n\nOnline regex tester\n - an online tool for testing a regular expression on a sample text typed by the user. Also provides quick reference sheet and interactive explanation of the expression being tested.\n\n\nGitHub repository\n - This course repository on GitHub. All the files are first uploaded on GitHub for hosting on this website adn on Moodle. \n\n\n\n\nSoftware\n\n\n\n\nAnaconda Python 2.7\n - Python powered open data science platform, which comes toegther with Jupyter notebooks, Spider IDE and some of the most popular Python libraries preinstalled. \n\n\nSublime Text Editor\n - A simple yet powerful (user friendly interface + amazing performance) text editor. During the course Sublime will be used for creating and editing HTML, CSS, XML and JSON documents.\n\n\nAtom text editor\n - Another text editor that can be used as an alternative to sublime. It has better user interface and more developed functionality, yet lacks the performance that Sublime offers.", 
            "title": "5. Resources"
        }, 
        {
            "location": "/resources/#chrome-extensions", 
            "text": "Selector Gadget  - A google chrome extension that helps to easily discover CSS selectors for the elements on the webpage.  JSON formatter  - A google chrome extension which makes the JSON representation indented and highlighted (when viewed directly inside the browser).  Regex search  - Another chrome extension which provides the opportunity of running a search on the webpage using regular extensions directly inside the borwser.", 
            "title": "Chrome extensions"
        }, 
        {
            "location": "/resources/#firefox-extensions", 
            "text": "JSON view  - A Firefox extension which makes the JSON representation indented and highlighted (when viewed directly inside the browser).", 
            "title": "Firefox extensions"
        }, 
        {
            "location": "/resources/#websites", 
            "text": "Online regex tester  - an online tool for testing a regular expression on a sample text typed by the user. Also provides quick reference sheet and interactive explanation of the expression being tested.  GitHub repository  - This course repository on GitHub. All the files are first uploaded on GitHub for hosting on this website adn on Moodle.", 
            "title": "Websites"
        }, 
        {
            "location": "/resources/#software", 
            "text": "Anaconda Python 2.7  - Python powered open data science platform, which comes toegther with Jupyter notebooks, Spider IDE and some of the most popular Python libraries preinstalled.   Sublime Text Editor  - A simple yet powerful (user friendly interface + amazing performance) text editor. During the course Sublime will be used for creating and editing HTML, CSS, XML and JSON documents.  Atom text editor  - Another text editor that can be used as an alternative to sublime. It has better user interface and more developed functionality, yet lacks the performance that Sublime offers.", 
            "title": "Software"
        }, 
        {
            "location": "/additional/", 
            "text": "Online Courses\n\n\n\n\nUsing Python to Access Web Data\n\n\nHTML, CSS, Python, Bootstrap, jQuery, JavaScript", 
            "title": "6. Additional"
        }, 
        {
            "location": "/additional/#online-courses", 
            "text": "Using Python to Access Web Data  HTML, CSS, Python, Bootstrap, jQuery, JavaScript", 
            "title": "Online Courses"
        }
    ]
}